{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todos\n",
    "\n",
    "- ~~Write Introduction~~\n",
    "- ~~Write Data formats~~\n",
    "- ~~Load data~~ \n",
    "- ~~Write function to print validation images~~\n",
    "- ~~Write function to add found boundaries to images~~\n",
    "- Find a prelearned convnet for sattelite images\n",
    "- Load it\n",
    "- Fix the weights\n",
    "- Train the head (softmax, 5 output neurons)\n",
    "- Unfix the weights\n",
    "- Train the whole classifier\n",
    "- Write function to create predictions\n",
    "<br>\n",
    "\n",
    "**Ideas**\n",
    "- Data Augmentation: Shifting, rotating and so on to increase amount of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one of their projects an insurance company wants to get an estimate of the assets of their clients. For this they want to scan satellite images for four different objects: ponds, pools, photovoltaic systems and trampolines.\n",
    "\n",
    "### Available data\n",
    "To solve the task the following data are available.\n",
    "\n",
    "**Training data**\n",
    "<br>\n",
    "The training data consist of five different groups. In the beginning the data themselves are not labels (as in labeled and already in a dataframe form). They are just .png and .jpg files in different subfolders. All training images are of size 256x256 pixels. The following five groups are available:\n",
    "- Trampoline images: 140 images that contain a trampoline.\n",
    "- Pond images: 9 images that contain a pond.\n",
    "- Pool images: 20 images that contain a pool.\n",
    "- Solar images: 37 images that contain a photovoltaic system.\n",
    "- Background images: 3110 images that do not contain any of the above-mentioned items.\n",
    "\n",
    "This sums up to 3316 labeled training patches. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Unlabeled training data**\n",
    "<br>\n",
    "In addition to the training patches, 20 validation images are available. They are of size 8000x8000 each and can be used for the manual validation of an approach.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Validation data**\n",
    "<br>\n",
    "The validation data set contains 3 images of size 8000x8000. In addition there is a csv file that contains the coordinates of an item and a surrounding bounding box of various items and their respective labels.\n",
    "Thus, one prediction should have the following shape (1,7) with column 1 as strings and the rest as integer values:\n",
    "|label|y_target|x_target|y_upper_left|x_upper_left|y_lower_right|x_lower_right\n",
    "|---|---|---|---|---|---|---|\n",
    "|trampoline|268|278|140|150|396|406|\n",
    "|...|\n",
    "\n",
    "<br>\n",
    "\n",
    "**Predictions** <br>\n",
    "A prediction should represent a bounding box around a target entity. One prediction consist of the label and the four coordinates that describe the bounding box. <br>\n",
    "<span style=\"color:red\">The bounding box must be of size 256 x 256</span>. <br>\n",
    "A prediction is considered correct if the predicted bounding box overlaps at least 50% with the ground truth bounding box. <br>\n",
    "This overlap is computed as the area of the intersection of the boxes divided by the union of the areas of the boxes: IoU = intersection_area / union_area:\n",
    "\n",
    "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2022/04/fig1.png\" alt=\"Intersection over Union for Object Detection | Baeldung on Computer Science\">\n",
    "\n",
    "Our predictions thus must each be of shape (1,5):\n",
    "|label|y_upper_left|x_upper_left|y_lower_right|x_lower_right\n",
    "|---|---|---|---|---|\n",
    "|trampoline|140|150|396|406|\n",
    "|...|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section loads the training patches into a numpy array and creates the corresponding label vector.\n",
    "The result are X_train, X_val, y_train and y_val. \n",
    "\n",
    "The images are converted to RGB values, which is why there are 3 channels in the training data.\n",
    "\n",
    "The training data sets are of dimension (number_of_instances x height x width x 3 channels). \n",
    "The label vectors only have one dimension (number_of_instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImagesToArray(path:str):\n",
    "    '''\n",
    "    Loads all .jpg and .png files from the specified directory.\\n\n",
    "    Each image will be converted into an array of size (height x width x channels).\\n\n",
    "    The return numpy array is of dimensions (numberOfImages x height x width x channels).\\n\n",
    "    '''\n",
    "    imagesArray = []\n",
    "\n",
    "    counter = 0\n",
    "    for file in os.scandir(path):\n",
    "        filepath = os.fsdecode(file)\n",
    "        if(filepath.endswith(\".jpg\") or filepath.endswith(\".png\")):\n",
    "                imgArray = np.array(Image.open(filepath))\n",
    "                imagesArray.append(imgArray)\n",
    "                counter += 1                  \n",
    "    return np.array(imagesArray)\n",
    "\n",
    "def loadTrainingDataAndLabels(path:str, subdirectories: list[str]):\n",
    "    '''\n",
    "    Loads the training data as numpy arrays and creates the corresponding labels.\\n\n",
    "    For this to work, the images should be under the folder <path> in separate subdirectories, one for each class.\\n\n",
    "    The labels will be inferred from the names of the subdirectories. \\n\n",
    "\n",
    "    Returns the training data as a numpy array with the dimensions (number_of_images x height x width x channels).\\n\n",
    "    Returns the labels as a numpy array with the dimensions (number_of_images).\n",
    "    '''\n",
    "\n",
    "    training_data = []\n",
    "    labels = []\n",
    "\n",
    "    for directory in subdirectories:\n",
    "        images_array = loadImagesToArray(os.path.join(path, directory))\n",
    "        training_data.extend(images_array)\n",
    "\n",
    "        labels.extend(np.full(len(images_array), directory))\n",
    "\n",
    "    training_data_array = np.array(training_data)\n",
    "    print(\"Shape of training_data: \", training_data_array.shape)\n",
    "    labels_array = np.array(labels)\n",
    "    print(\"Shape of labels: \", labels_array.shape)\n",
    "    \n",
    "    return training_data_array, labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training_data:  (3316, 256, 256, 3)\n",
      "Shape of labels:  (3316,)\n"
     ]
    }
   ],
   "source": [
    "training_data, labels = loadTrainingDataAndLabels(\"data/training_patches/\", [\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_data, labels, test_size=0.33, random_state=1, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (2221, 256, 256, 3)\n",
      "Shape of y_train:  (2221,)\n",
      "Shape of X_val:  (1095, 256, 256, 3)\n",
      "Shape of y_val:  (1095,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_val: \", X_val.shape)\n",
    "print(\"Shape of y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction with ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Functional)    (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 22,378,789\n",
      "Trainable params: 571,013\n",
      "Non-trainable params: 21,807,776\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.InceptionV3(weights=\"imagenet\", include_top=False, pooling=\"avg\", input_shape=(256, 256, 3))\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Functional)    (None, 2048)              21802784  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 22,378,789\n",
      "Trainable params: 571,013\n",
      "Non-trainable params: 21,807,776\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2221, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import pandas as pd\n",
    "\n",
    "X_train_preprocessed = preprocess_input(np.copy(X_train))\n",
    "X_val_preprocessed = preprocess_input(np.copy(X_val))\n",
    "\n",
    "# One-Hot Encoding y_train\n",
    "y_train_encoded = pd.get_dummies(y_train)\n",
    "y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 23s 133ms/step - loss: 1.8000 - accuracy: 0.3238 - val_loss: 1.2141 - val_accuracy: 0.7130\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 5s 82ms/step - loss: 1.1530 - accuracy: 0.6041 - val_loss: 0.6702 - val_accuracy: 0.9238\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 5s 82ms/step - loss: 0.6636 - accuracy: 0.8178 - val_loss: 0.3771 - val_accuracy: 0.9327\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 5s 82ms/step - loss: 0.4258 - accuracy: 0.9034 - val_loss: 0.2757 - val_accuracy: 0.9417\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 5s 82ms/step - loss: 0.3063 - accuracy: 0.9344 - val_loss: 0.2616 - val_accuracy: 0.9372\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 5s 82ms/step - loss: 0.2596 - accuracy: 0.9399 - val_loss: 0.2568 - val_accuracy: 0.9327\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 5s 82ms/step - loss: 0.2438 - accuracy: 0.9429 - val_loss: 0.2542 - val_accuracy: 0.9372\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 5s 82ms/step - loss: 0.2173 - accuracy: 0.9474 - val_loss: 0.2599 - val_accuracy: 0.9462\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 5s 83ms/step - loss: 0.1770 - accuracy: 0.9545 - val_loss: 0.2653 - val_accuracy: 0.9417\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 5s 85ms/step - loss: 0.1906 - accuracy: 0.9550 - val_loss: 0.2708 - val_accuracy: 0.9417\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_preprocessed, \n",
    "                    y_train_encoded, \n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoded predictions  (2221, 5)\n",
      "Shape of validation data features (1095, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions_train_encoded = pd.DataFrame(model.predict(X_train_preprocessed), columns=[\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"])\n",
    "predictions_val_encoded = pd.DataFrame(model.predict(X_val_preprocessed), columns=[\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"])\n",
    "\n",
    "print(\"Shape of encoded predictions \", predictions_train_encoded.shape)\n",
    "print(\"Shape of validation data features\", predictions_val_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation data:  0.9616438356164384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    background\n",
       "1    background\n",
       "2    background\n",
       "3    background\n",
       "4    background\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions_train = predictions_train_encoded.idxmax(1)\n",
    "predictions_val = predictions_val_encoded.idxmax(1)\n",
    "\n",
    "print(\"Accuracy on validation data: \", accuracy_score(y_val, predictions_val))\n",
    "predictions_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_val, predictions_val, labels=[\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"])\n",
    "\n",
    "color = 'red'\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "matrix = ConfusionMatrixDisplay.from_predictions(y_val, predictions_val, labels=[\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"], cmap=plt.cm.Blues, xticks_rotation='vertical', ax=ax)\n",
    "matrix.ax_.set_title('Confusion Matrix', color=color)\n",
    "plt.xlabel('Predicted Label', color=color)\n",
    "plt.ylabel('True Label', color=color)\n",
    "plt.gcf().axes[0].tick_params(colors=color)\n",
    "plt.gcf().axes[1].tick_params(colors=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras import Model\n",
    "import time\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictions(path:str, convnet:keras.Model, stepSize:int, windowSize:tuple[int, int]):\n",
    "    '''\n",
    "    Traverses a folder that contains images for which predictions should be made.\\n\n",
    "    Creates a separate prediction CSV file for each image.\n",
    "\n",
    "    @path - The path containing the images for which predictions should be created.\n",
    "    '''\n",
    "\n",
    "    # For each image in path\n",
    "        # Perform sliding Window approach\n",
    "            # For each slide\n",
    "            # Store x_upper_left, y_upper_left, x_lower_right, y_lower_right\n",
    "            # Run image through convnet\n",
    "            # Run classifier on output\n",
    "            # If prediction != 'background'\n",
    "            # Store prediction in temp array\n",
    "        # Run non-max suppression to filter predictions\n",
    "        # Store predictions in csv\n",
    "    for file in os.scandir(path):\n",
    "        filepath = os.fsdecode(file)\n",
    "        \n",
    "        if((\"annotated\" in filepath) or not (filepath.endswith(\".jpg\") or filepath.endswith(\".png\"))):\n",
    "           continue\n",
    "        \n",
    "        createPredictionsForImage(filepath=filepath, convnet=convnet, stepSize=stepSize, windowSize=windowSize)\n",
    "\n",
    "\n",
    "def createPredictionsForImage(filepath:str, convnet:keras.Model, stepSize:int, windowSize:tuple[int,int]):\n",
    "    '''\n",
    "    Creates the prediction CSV for one image.\n",
    "    '''\n",
    "\n",
    "    print(\"\\nCreating predictions for file: \", filepath)\n",
    "    create_predictions_start_time = time.time()\n",
    "    #image = Image.open(filepath)\n",
    "    imgArray = np.array(Image.open(filepath))\n",
    "    \n",
    "    patch_coordinates= []  \n",
    "    preprocessed_patches = []\n",
    "    counter = 0\n",
    "    patch_preprocessing_start_time = time.time()\n",
    "    \n",
    "    print(\"Starting sliding window to create patches of size: \", windowSize[0], \"x\", windowSize[1], \".\")\n",
    "    for(x,y,patch) in sliding_window(imageArray=imgArray, stepSize=stepSize, windowSize=windowSize):\n",
    "        if counter > 0 and counter%10000 == 0:\n",
    "            print(\"Still processing, reached patch\", counter)\n",
    "            print(\"Execution time for the last 10.000 patches: \", time.time()-patch_preprocessing_start_time, \" seconds.\")\n",
    "            patch_preprocessing_start_time = time.time()\n",
    "            print(\"Processing continues...\")\n",
    "        \n",
    "        # Skip if the size of a patch doesn't match the specified windowSize\n",
    "        if patch.shape[0] != windowSize[0] or patch.shape[1] != windowSize[1]:\n",
    "            continue\n",
    "    \n",
    "        # Save coordinates which are needed for a prediction\n",
    "        x_upper_left = x\n",
    "        y_upper_left = y\n",
    "        x_lower_right = x+windowSize[0]\n",
    "        y_lower_right = y+windowSize[1]\n",
    "        x_center = x+128\n",
    "        y_center = y+128\n",
    "\n",
    "        # Run the patch through the classification\n",
    "        preprocessed_patch = preprocess_input(patch)\n",
    "        preprocessed_patches.append(preprocessed_patch)\n",
    "        patch_coordinates.append([y_upper_left, x_upper_left, y_lower_right, x_lower_right])\n",
    "        counter +=1\n",
    "    \n",
    "    print(\"Finished preprocessing of the patches.\")\n",
    "    preprocessed_patches = np.array(preprocessed_patches)\n",
    "    patch_coordinates = np.array(patch_coordinates)\n",
    "    print(\"Shape of preprocessed patches: \", preprocessed_patches.shape)\n",
    "    print(\"Shape of patch coordinates: \", patch_coordinates.shape, \"\\n\")\n",
    "\n",
    "    # Get all predictions\n",
    "    print(\"Running patches through ConvNet and using classifier to predict labels...\")\n",
    "    prediction_start_time = time.time()\n",
    "    predicted_labels_encoded = pd.DataFrame(convnet.predict(preprocessed_patches), columns=[\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"])\n",
    "    predicted_labels= predicted_labels_encoded.idxmax(1)\n",
    "    \n",
    "    # Create a column with the score for the predicted class\n",
    "    highest_scores = predicted_labels_encoded[[\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"]].max(axis=1)\n",
    "    print(\"Shape of highest_scores: \", highest_scores.shape)\n",
    "\n",
    "    print(\"Finished predictions, execution time: \", time.time()-prediction_start_time, \" seconds.\\n\")\n",
    "    \n",
    "    print(\"Shape of patch_coordinates: \", patch_coordinates.shape)\n",
    "\n",
    "    # Combining patch coordinates and predictions\n",
    "    predictions_array=np.c_[highest_scores, predicted_labels, patch_coordinates]\n",
    "\n",
    "    print(\"Shape of combined predictions array (unfiltered): \", predictions_array.shape)\n",
    "\n",
    "    predictions_dataframe = pd.DataFrame(data=predictions_array, columns=[\"score\", \"label\", \"y_upper_left\", \"x_upper_left\", \"y_lower_right\", \"x_lower_right\"])\n",
    "    # Filter all predictions that contain the label \"background\"\n",
    "    predictions_dataframe = predictions_dataframe[predictions_dataframe.label != \"background\"]\n",
    "    print(\"Description of the predictions dataframe: \", predictions_dataframe.describe())\n",
    "\n",
    "    # Save prediction to csv\n",
    "    savePredictionToCsv(predictionDataframe=predictions_dataframe, filepath=filepath)\n",
    "    print(\"Saved predictions for file: \", filepath, \"\\n\")\n",
    "    print(\"Elapsed time: \", time.time()-create_predictions_start_time, \" seconds.\\n\")\n",
    "\n",
    "    \n",
    "def sliding_window(imageArray, stepSize:int, windowSize:tuple[int, int]=(256,256)):\n",
    "    for y in range(0, imageArray.shape[0], stepSize):\n",
    "\t    for x in range(0, imageArray.shape[1], stepSize):\n",
    "\t\t\t# yield the current window\n",
    "\t\t    yield (x, y, imageArray[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating predictions for file:  ./data/single_validation_image/DQIMQN.png\n",
      "Starting sliding window to create patches of size:  256 x 256 .\n",
      "Still processing, reached patch 10000\n",
      "Execution time for the last 10.000 patches:  3.8803608417510986  seconds.\n",
      "Processing continues...\n",
      "Finished preprocessing of the patches.\n",
      "Shape of preprocessed patches:  (14884, 256, 256, 3)\n",
      "Shape of patch coordinates:  (14884, 4) \n",
      "\n",
      "Running patches through ConvNet and using classifier to predict labels...\n",
      "Shape of highest_scores:  (14884,)\n",
      "Finished predictions, execution time:  30.97353172302246  seconds.\n",
      "\n",
      "Shape of patch_coordinates:  (14884, 4)\n",
      "Shape of combined predictions array (unfiltered):  (14884, 6)\n",
      "Description of the predictions dataframe:               score       label  y_upper_left  x_upper_left  y_lower_right  \\\n",
      "count   116.000000         116           116           116            116   \n",
      "unique  116.000000           1            46            55             46   \n",
      "top       0.443067  trampoline            64          5888            320   \n",
      "freq      1.000000         116             8             6              8   \n",
      "\n",
      "        x_lower_right  \n",
      "count             116  \n",
      "unique             55  \n",
      "top              6144  \n",
      "freq                6  \n",
      "Saved predictions for file:  ./data/single_validation_image/DQIMQN.png \n",
      "\n",
      "Elapsed time:  43.77367162704468  seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "makePredictions(\"./data/single_validation_image/\", convnet=model, stepSize=64, windowSize=(256,256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing non-max suppresion on file  ./data/single_validation_image/DQIMQN.csv\n",
      "OriginalPredictions shape:  [4.43066686e-01            nan 0.00000000e+00 1.92000000e+03\n",
      " 2.56000000e+02 2.17600000e+03]\n",
      "Shape of coordinates:  (117, 4)\n",
      "Shape of labels:  (117,)\n",
      "(16,)\n",
      "['trampoline' 'trampoline' 'trampoline' ' pool' 'trampoline' 'trampoline'\n",
      " 'trampoline' 'trampoline' 'trampoline' 'trampoline' 'trampoline'\n",
      " 'trampoline' 'trampoline' 'trampoline' 'trampoline' 'trampoline']\n",
      "Shape of selected labels:  (16,)\n",
      "Shape of selected boxes:  (16, 4)\n",
      "         label y_upper_left x_upper_left y_lower_right x_lower_right\n",
      "0   trampoline           64         2240           320          2496\n",
      "1   trampoline         1088         5632          1344          5888\n",
      "2   trampoline          128         5248           384          5504\n",
      "3         pool           66         1922           320          2176\n",
      "4   trampoline         2176         5056          2432          5312\n",
      "5   trampoline          640         2240           896          2496\n",
      "6   trampoline          832         5952          1088          6208\n",
      "7   trampoline         1536         1856          1792          2112\n",
      "8   trampoline         2752         6208          3008          6464\n",
      "9   trampoline         4992         4800          5248          5056\n",
      "10  trampoline          256         2816           512          3072\n",
      "11  trampoline         1728         1344          1984          1600\n",
      "12  trampoline          640         5632           896          5888\n",
      "13  trampoline         1408         4288          1664          4544\n",
      "14  trampoline         4032         6912          4288          7168\n",
      "15  trampoline         6528         2048          6784          2304\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def nonMaxSuppressBoundingBoxes(path:str, iou_threshold:float, score_threshold:float):\n",
    "    '''\n",
    "    Loads the csv files from the path and performs the non-max-suppression to reduce the amount of predictions to one per detected object.\\n\n",
    "    '''\n",
    "\n",
    "    for file in os.scandir(path):\n",
    "        filepath = os.fsdecode(file)\n",
    "        \n",
    "        if(not(filepath.endswith(\".csv\")) or (\"suppressed\" in filepath)):\n",
    "           continue\n",
    "\n",
    "        print(\"Performing non-max suppresion on file \", filepath)\n",
    "        originalPredictions = np.genfromtxt(filepath, delimiter=',', skip_header=1)\n",
    "        originalPredictions_df= pd.read_csv(filepath)\n",
    "        print(\"OriginalPredictions shape: \", originalPredictions[0])\n",
    "\n",
    "        labels = originalPredictions_df[\"label\"]\n",
    "        coordinates = originalPredictions[:, 2:6].astype(int)\n",
    "        scores = originalPredictions[:,0]\n",
    "        print(\"Shape of coordinates: \", coordinates.shape)\n",
    "        print(\"Shape of labels: \", scores.shape)\n",
    "       \n",
    "        selectedBoxes_indices = tf.image.non_max_suppression(boxes=coordinates, scores=scores, max_output_size=200, iou_threshold=iou_threshold, score_threshold=score_threshold )\n",
    "        print(selectedBoxes_indices.shape)\n",
    "\n",
    "        selected_boxes = tf.gather(coordinates, selectedBoxes_indices).numpy()\n",
    "        selected_labels = np.array([x.numpy().decode() for x in tf.gather(labels, selectedBoxes_indices)])\n",
    "        selected_scores = tf.gather(scores, selectedBoxes_indices).numpy()\n",
    "        print(selected_labels)\n",
    "        print(\"Shape of selected labels: \", selected_labels.shape)\n",
    "        print(\"Shape of selected boxes: \", selected_boxes.shape)\n",
    "\n",
    "        predictions = pd.DataFrame(np.c_[selected_labels, selected_boxes], columns=[\"label\", \"y_upper_left\", \"x_upper_left\", \"y_lower_right\", \"x_lower_right\"])\n",
    "        print(predictions)\n",
    "        new_filepath =  os.path.splitext(filepath)[0]+\"_suppressed.csv\"\n",
    "        predictions.to_csv(new_filepath, sep=\",\", index=False)\n",
    "        \n",
    "\n",
    "nonMaxSuppressBoundingBoxes(\"./data/single_validation_image/\", iou_threshold=0.5, score_threshold=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "preprocessed_patches = None\n",
    "del preprocessed_patches\n",
    "patch_coordinates = None\n",
    "del patch_coordinates\n",
    "X_train = None\n",
    "del X_train\n",
    "X_val = None \n",
    "del X_val\n",
    "y_train = None\n",
    "del y_train\n",
    "y_val = None\n",
    "training_data = None\n",
    "del training_data\n",
    "X_train_preprocessed = None\n",
    "del X_train_preprocessed\n",
    "predictions_array = None\n",
    "del predictions_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains code that is needed to create predictions, save them and to print the training / validation images - either with or without the predicted bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictionToCsv(predictionDataframe: pd.DataFrame, filepath:str):\n",
    "    '''\n",
    "    Saves a dataframe containing the prediction for a single image to a CSV file.\n",
    "\n",
    "    @predictionDataFrame - The dataframe that contains the predictions and should be saved.\\n\n",
    "    @path - The path under which the CSV file should be saved.\n",
    "    @filename - The name under which the CSV file should be saved.\n",
    "    '''\n",
    "    filepath =  os.path.splitext(filepath)[0]+\".csv\"\n",
    "    predictionDataframe.to_csv(filepath, sep=\",\", index=False)\n",
    "\n",
    "def makeFakePredictions(path:str):\n",
    "    '''\n",
    "    Traverses a folder that contains images for which predictions should be made.\\n\n",
    "    Creates a separate prediction CSV file for each image.\n",
    "\n",
    "    @path - The path containing the images for which predictions should be created.\n",
    "    '''\n",
    "    fake_predictions = {\"label\":[\"something\"], \"y_upper_left\": [100], \"x_upper_left\":[100], \"y_lower_right\":[356], \"x_lower_right\":[356]}\n",
    "    dataframe = pd.DataFrame(data=fake_predictions)\n",
    "    counter = 0\n",
    "    for file in os.scandir(path):\n",
    "        filepath = os.fsdecode(file)\n",
    "\n",
    "        if(not (filepath.endswith(\".jpg\") or filepath.endswith(\".png\")) or (\"annotated\" in filepath)):\n",
    "            continue\n",
    "        print(\"Making prediction for file: \", file.name) \n",
    "        savePredictionToCsv(dataframe, filepath)\n",
    "        counter += 1\n",
    "    print(\"Created \",counter, \" predictions.\")\n",
    "\n",
    "def saveOrPrintImages(path:str, print_to_output:bool=False, valBoundingBoxes: bool = False, thickness: int = 2, saveImagesPath:str=\"\"):\n",
    "    '''\n",
    "    Loads and saves all .jpg and .png files from the specified directory.\\n\n",
    "\n",
    "    @print - If set to True, will print the (annotated) images to the output (takes some time). Default value is False.\n",
    "\n",
    "    @valBoundingBoxes If set to true, the method will load the bounding box data from the csv files. The default for this is False.\\n\n",
    "  \n",
    "    @thickness parameter determines how thick the bounding boxes are drawn on the image (width of the line in pixels). The default value is 2.\\n\n",
    "\n",
    "    @saveImagesPath If specified will save the drawn images at the specified path.\n",
    "\n",
    "    '''\n",
    "    for file in os.scandir(path):\n",
    "        filepath = os.fsdecode(file)\n",
    "       \n",
    "\n",
    "        if(not (filepath.endswith(\".jpg\") or filepath.endswith(\".png\"))  or (\"annotated\" in filepath)):\n",
    "            continue\n",
    "\n",
    "        print(\"Saving / printing file: \",os.path.splitext(file.name)[0],\"_annotated.jpg\")\n",
    "        image = cv.imread(filepath) \n",
    "        \n",
    "        if(valBoundingBoxes):\n",
    "            image = addBoundingBoxesFromCsv(image, filepath, thickness)\n",
    "\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        # Convert to PIL format\n",
    "        image = Image.fromarray(image)\n",
    "        if(saveImagesPath):\n",
    "            image.save(saveImagesPath+\"/\"+ os.path.splitext(file.name)[0]+\"_annotated.jpg\", \"JPEG\")\n",
    "\n",
    "        if(print_to_output):\n",
    "            image.show()\n",
    "        \n",
    "\n",
    "def addBoundingBoxesFromCsv(image: cv.Mat, filepath:str, thickness: int) -> cv.Mat:\n",
    "    '''\n",
    "    Uses a csv file containing the predictions for a single image to draw bounding boxes into an image.\\n\n",
    "\n",
    "    Performs the changes in place and returns the modified image.\n",
    "    The image parameter specifies the image to be modified.\n",
    "    The filepath must contain the path of the csv file that contains the bounding box predictions for this image.\n",
    "    '''\n",
    "    csvPath = os.path.splitext(filepath)[0]+\"_suppressed.csv\"\n",
    "    boundingBoxesDataframe = pd.read_csv(csvPath, delimiter=\",\", header=0 )\n",
    "\n",
    "    for index,boundingBox in boundingBoxesDataframe.iterrows():\n",
    "        label = boundingBox[\"label\"]\n",
    "        start_point = (int(boundingBox[\"x_upper_left\"]),int(boundingBox[\"y_upper_left\"]))\n",
    "        end_point = (int(boundingBox[\"x_lower_right\"]), int(boundingBox[\"y_lower_right\"]))\n",
    "\n",
    "        color:str\n",
    "        \n",
    "        if(label== \"background\"):\n",
    "            color=(255,255,255) # (BGR) White\n",
    "        elif(label == \"pool\"):\n",
    "            color=(255,255,0) # (BGR) Cyan\n",
    "        elif(label == \"pond\"):\n",
    "            color=(0,128,0) # (BGR) Green\n",
    "        elif(label == \"solar\"):\n",
    "            color=(255,0,0) # (BGR) Blue\n",
    "        elif(label == \"trampoline\"):\n",
    "            color=(0,255,255) # (BGR) Yellow\n",
    "        else:\n",
    "            color=(180,105,255) # (BGR) Pink if no label present\n",
    "        image = cv.rectangle(image, start_point, end_point, color, thickness=thickness)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation (Custom Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving / printing file:  DQIMQN _annotated.jpg\n"
     ]
    }
   ],
   "source": [
    "saveOrPrintImages(path=\"./data/single_validation_image/\", print_to_output=False, valBoundingBoxes=True,saveImagesPath=\"./data/single_validation_image/\", thickness=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, glob, cv2, random, os\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def calc_performance(gt_path, pred_path, image_name=None, verbose=0):\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "\n",
    "    # Create default performance values\n",
    "    performances = {\n",
    "        'file': image_name,\n",
    "        'tp': 0,\n",
    "        'fn': 0,\n",
    "        'fp': 0,\n",
    "        'f1': 0,\n",
    "    }\n",
    "\n",
    "    ## Load ground truth\n",
    "    with open(gt_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            row = {k: int(row[k]) if k != 'label' else row[k] for k in row.keys()}\n",
    "            ground_truth.append(row)\n",
    "\n",
    "    ## load predictions if path exists\n",
    "    if os.path.exists(pred_path):\n",
    "        with open(pred_path) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                row = {k: int(row[k]) if k != 'label' else row[k] for k in row.keys()}\n",
    "                predictions.append(row)\n",
    "\n",
    "    # Number of false positives equals number of left predictions\n",
    "    performances['fp'] = max(len(predictions) - len(ground_truth), 0)\n",
    "\n",
    "    for j, gt in enumerate(ground_truth):\n",
    "        gt_box = Polygon([(gt['y_upper_left'],  gt['x_upper_left']),\n",
    "                          (gt['y_upper_left'],  gt['x_lower_right']),\n",
    "                          (gt['y_lower_right'], gt['x_lower_right']),\n",
    "                          (gt['y_lower_right'], gt['x_upper_left'])])\n",
    "\n",
    "        if gt_box.area != (256. * 256.):\n",
    "            print(f'### Warning {j}: false ground truth shape of {gt_box.area} detected in {image_name}!')\n",
    "            print(gt['y_lower_right'] - gt['y_upper_left'], gt['x_lower_right'] - gt['x_upper_left'])\n",
    "\n",
    "        best_found_iou = (None, 0.) # (idx, IoU)\n",
    "        for i, pred in enumerate(predictions):\n",
    "            if gt['label'] == pred['label']:\n",
    "                pred_box = Polygon([(pred['y_upper_left'],  pred['x_upper_left']),\n",
    "                                    (pred['y_upper_left'],  pred['x_lower_right']),\n",
    "                                    (pred['y_lower_right'], pred['x_lower_right']),\n",
    "                                    (pred['y_lower_right'], pred['x_upper_left'])])\n",
    "\n",
    "                if pred_box.area != (256. * 256.):\n",
    "                    print(f'### Warning {i}: false predicted shape of {pred_box.area} detected in {image_name}!')\n",
    "                    print(pred['y_lower_right'] - pred['y_upper_left'], pred['x_lower_right'] - pred['x_upper_left'])\n",
    "\n",
    "                ## Calculate IoU\n",
    "                next_iou = (gt_box.intersection(pred_box).area + 1) / (gt_box.union(pred_box).area + 1)\n",
    "\n",
    "                # If the next found IoU is larger than the previous found IoU -> override\n",
    "                if next_iou > best_found_iou[1]:\n",
    "                    best_found_iou = (i, next_iou)\n",
    "\n",
    "        ## Append metric. If IoU is larger 0.5, then its a true positive, else false negative\n",
    "        if best_found_iou[0] is not None and best_found_iou[1] >= 0.5:\n",
    "            del predictions[best_found_iou[0]] # Remove prediction from list!\n",
    "            performances['tp'] += 1 # Increase number of True Positives\n",
    "            if verbose == 1:\n",
    "                print(f'Found correct prediction with IoU of {round(best_found_iou[1], 3)} and label {gt[\"label\"]}!')\n",
    "        else:\n",
    "            performances['fn'] += 1 # Increase number of False Negatives\n",
    "            if verbose == 1:\n",
    "                print(f'Found false prediction with IoU of {round(best_found_iou[1], 3)} and label {gt[\"label\"]}!')\n",
    "\n",
    "    ## Calculate F1-Score\n",
    "    performances['f1'] = (performances['tp'] + 1e-8) / \\\n",
    "                                    (performances['tp'] + 0.5 * (performances['fp'] + performances['fn']) + 1e-8)\n",
    "    return performances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'single_validation_image\\\\DQIMQN.png', 'tp': 11, 'fn': 19, 'fp': 0, 'f1': 0.5365853660797145}\n"
     ]
    }
   ],
   "source": [
    "path = './data/single_validation_image/' # Change if needed\n",
    "\n",
    "## Iterate over all validation images\n",
    "for image_path in glob.glob(path + '/*.png'):\n",
    "    image_name = image_path.split('/')[-1]\n",
    "    gt_path = image_path[:-4] + '_echte.csv' # Ground Truth path\n",
    "    pred_path = image_path[:-4] + '_suppressed.csv' # Prediction path\n",
    "    performance = calc_performance(gt_path, pred_path, image_name)\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation (Unlabeled Training Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section prints the unlabeled training images along with bounding boxes that are retrieved from the corresponding CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeFakePredictions(path=\"./data/unlabeled_training_data/\")\n",
    "saveOrPrintImages(path=\"./data/unlabeled_training_data/\", print_to_output=False, valBoundingBoxes=True,saveImagesPath=\"./data/unlabeled_training_data/\", thickness=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section prints the public test data images along with their respective predictions / bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving / printing file:  DQIMQN _annotated.jpg\n",
      "Saving / printing file:  L7CT2I _annotated.jpg\n",
      "Saving / printing file:  UDPYYD _annotated.jpg\n"
     ]
    }
   ],
   "source": [
    "# Print images from the validation data and use the corresponding csv files to add bounding boxes of classified entities\n",
    "saveOrPrintImages(\"data/validation_data/\", print_to_output=False, valBoundingBoxes=True, thickness=5, saveImagesPath=\"./data/validation_data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a578b60687b99a82ca78389cf5818de06cdae5f0e0fb7a238655a79bce45af2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
