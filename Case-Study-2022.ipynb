{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2022 - Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'drawImages' from '/Users/alex/Documents/GitHub/da2-group-3/drawImages.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload \n",
    "import augmentData\n",
    "import loadAndStoreData\n",
    "import processData\n",
    "import drawImages\n",
    "reload(augmentData)\n",
    "reload(loadAndStoreData)\n",
    "reload(processData)\n",
    "reload(drawImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the training data is augmented. The function allows to choose the classes for which the augmentations should be done.\n",
    "It also allows to define the augmentation techniques that are used. \n",
    "\n",
    "For each augmentation technique a new subfolder is created. Each subfolder contains the augmented images of the classes chosen.\n",
    "Depending on the augmentation techniques chosen, this process may neeed a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  training_patches_brightnessdown/background  Created \n",
      "Directory  training_patches_brightnessdown/background  already exists\n",
      "Directory  training_patches_brightnessdown/solar  Created \n",
      "Directory  training_patches_brightnessdown/solar  already exists\n",
      "Directory  training_patches_brightnessdown/ponds  Created \n",
      "Directory  training_patches_brightnessdown/ponds  already exists\n",
      "Directory  training_patches_brightnessdown/trampoline  Created \n",
      "Directory  training_patches_brightnessdown/trampoline  already exists\n",
      "Directory  training_patches_brightnessdown/pools  Created \n",
      "Directory  training_patches_brightnessdown/pools  already exists\n",
      "Directory  training_patches_brightnessup/background  already exists\n",
      "Directory  training_patches_brightnessup/background  already exists\n",
      "Directory  training_patches_brightnessup/solar  Created \n",
      "Directory  training_patches_brightnessup/solar  already exists\n",
      "Directory  training_patches_brightnessup/ponds  Created \n",
      "Directory  training_patches_brightnessup/ponds  already exists\n",
      "Directory  training_patches_brightnessup/trampoline  Created \n",
      "Directory  training_patches_brightnessup/trampoline  already exists\n",
      "Directory  training_patches_brightnessup/pools  Created \n",
      "Directory  training_patches_brightnessup/pools  already exists\n",
      "Directory  training_patches_combined/background  already exists\n",
      "Directory  training_patches_combined/background  already exists\n",
      "Directory  training_patches_combined/solar  already exists\n",
      "Directory  training_patches_combined/solar  already exists\n",
      "Directory  training_patches_combined/ponds  already exists\n",
      "Directory  training_patches_combined/ponds  already exists\n",
      "Directory  training_patches_combined/trampoline  already exists\n",
      "Directory  training_patches_combined/trampoline  already exists\n",
      "Directory  training_patches_combined/pools  already exists\n",
      "Directory  training_patches_combined/pools  already exists\n",
      "Directory  training_patches_down/background  already exists\n",
      "Directory  training_patches_down/background  already exists\n",
      "Directory  training_patches_down/solar  Created \n",
      "Directory  training_patches_down/solar  already exists\n",
      "Directory  training_patches_down/ponds  Created \n",
      "Directory  training_patches_down/ponds  already exists\n",
      "Directory  training_patches_down/trampoline  Created \n",
      "Directory  training_patches_down/trampoline  already exists\n",
      "Directory  training_patches_down/pools  Created \n",
      "Directory  training_patches_down/pools  already exists\n",
      "Directory  training_patches_up/background  already exists\n",
      "Directory  training_patches_up/background  already exists\n",
      "Directory  training_patches_up/solar  Created \n",
      "Directory  training_patches_up/solar  already exists\n",
      "Directory  training_patches_up/ponds  Created \n",
      "Directory  training_patches_up/ponds  already exists\n",
      "Directory  training_patches_up/trampoline  Created \n",
      "Directory  training_patches_up/trampoline  already exists\n",
      "Directory  training_patches_up/pools  Created \n",
      "Directory  training_patches_up/pools  already exists\n",
      "Directory  training_patches_right/background  already exists\n",
      "Directory  training_patches_right/background  already exists\n",
      "Directory  training_patches_right/solar  Created \n",
      "Directory  training_patches_right/solar  already exists\n",
      "Directory  training_patches_right/ponds  Created \n",
      "Directory  training_patches_right/ponds  already exists\n",
      "Directory  training_patches_right/trampoline  Created \n",
      "Directory  training_patches_right/trampoline  already exists\n",
      "Directory  training_patches_right/pools  Created \n",
      "Directory  training_patches_right/pools  already exists\n",
      "Directory  training_patches_left/background  already exists\n",
      "Directory  training_patches_left/background  already exists\n",
      "Directory  training_patches_left/solar  Created \n",
      "Directory  training_patches_left/solar  already exists\n",
      "Directory  training_patches_left/ponds  Created \n",
      "Directory  training_patches_left/ponds  already exists\n",
      "Directory  training_patches_left/trampoline  Created \n",
      "Directory  training_patches_left/trampoline  already exists\n",
      "Directory  training_patches_left/pools  Created \n",
      "Directory  training_patches_left/pools  already exists\n",
      "Directory  training_patches_rotation/background  already exists\n",
      "Directory  training_patches_rotation/background  already exists\n",
      "Directory  training_patches_rotation/solar  Created \n",
      "Directory  training_patches_rotation/solar  already exists\n",
      "Directory  training_patches_rotation/ponds  Created \n",
      "Directory  training_patches_rotation/ponds  already exists\n",
      "Directory  training_patches_rotation/trampoline  Created \n",
      "Directory  training_patches_rotation/trampoline  already exists\n",
      "Directory  training_patches_rotation/pools  Created \n",
      "Directory  training_patches_rotation/pools  already exists\n",
      "Directory  training_patches_zoom/background  already exists\n",
      "Directory  training_patches_zoom/background  already exists\n",
      "Directory  training_patches_zoom/solar  Created \n",
      "Directory  training_patches_zoom/solar  already exists\n",
      "Directory  training_patches_zoom/ponds  Created \n",
      "Directory  training_patches_zoom/ponds  already exists\n",
      "Directory  training_patches_zoom/trampoline  Created \n",
      "Directory  training_patches_zoom/trampoline  already exists\n",
      "Directory  training_patches_zoom/pools  Created \n",
      "Directory  training_patches_zoom/pools  already exists\n"
     ]
    }
   ],
   "source": [
    "augmentData.performDataAugmentation(\n",
    "    directory=\"training_patches/\", \n",
    "    categories=[\"ponds\", \"pools\",\"solar\",\"trampoline\"], \n",
    "    augmentations=[\"rotate_images\", \"move_images\", \"zoom_images\", \"change_brightness\", \"combine_augmentations\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section loads the training patches into a numpy array and creates the corresponding label vector.\n",
    "The result are X_train, X_val, y_train and y_val. \n",
    "\n",
    "The images are converted to RGB values, which is why there are 3 channels in the training data.\n",
    "\n",
    "The training data sets are of dimension (number_of_instances x height x width x 3 channels). \n",
    "The label vectors only have one dimension (number_of_instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training_data array:  (3316, 256, 256, 3)\n",
      "Shape of labels array:  (3316,)\n",
      "Shape of training_data array:  (206, 256, 256, 3)\n",
      "Shape of labels array:  (206,)\n",
      "Shape of training_data array:  (206, 256, 256, 3)\n",
      "Shape of labels array:  (206,)\n",
      "Shape of training_data array:  (206, 256, 256, 3)\n",
      "Shape of labels array:  (206,)\n",
      "Shape of training_data array:  (618, 256, 256, 3)\n",
      "Shape of labels array:  (618,)\n",
      "Shape of training_data array:  (206, 256, 256, 3)\n",
      "Shape of labels array:  (206,)\n",
      "Shape of final training_data:  (4758, 256, 256, 3)\n",
      "Shape of final labels:  (4758,)\n"
     ]
    }
   ],
   "source": [
    "training_data, labels = loadAndStoreData.loadTrainingDataAndLabels(\n",
    "    folders=[\n",
    "        \"training_patches/\",\n",
    "        \"training_patches_brightnessdown\",\n",
    "        \"training_patches_brightnessup\",\n",
    "        \"training_patches_combined\",\n",
    "        #\"training_patches_down\",\n",
    "        #\"training_patches_left\",\n",
    "        #\"training_patches_right\",\n",
    "        \"training_patches_rotation\",\n",
    "        #\"training_patches_up\",\n",
    "        \"training_patches_zoom\"\n",
    "    ], \n",
    "    subdirectories=[\"background\", \"ponds\", \"pools\", \"solar\", \"trampoline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3187, 256, 256, 3)\n",
      "(3187,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels_categorical = processData.labels_to_categorical(labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(training_data, labels_categorical, test_size=0.33, random_state=1, stratify=labels)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = processData.encodeLabels(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 10)      280       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 10)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 163840)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 163840)            655360    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               41943296  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 42,642,205\n",
      "Trainable params: 42,313,629\n",
      "Non-trainable params: 328,576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import InputLayer, Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(InputLayer(input_shape=(256,256,3)))\n",
    "model.add(Conv2D(filters=10, kernel_size=(3,3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 36s 763ms/step - loss: 1.8317 - accuracy: 0.3128 - val_loss: 10.0967 - val_accuracy: 0.0690\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 76s 2s/step - loss: 1.1943 - accuracy: 0.5858 - val_loss: 1.1051 - val_accuracy: 0.7492\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 57s 1s/step - loss: 0.7636 - accuracy: 0.7671 - val_loss: 0.7684 - val_accuracy: 0.8056\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 36s 803ms/step - loss: 0.5483 - accuracy: 0.8347 - val_loss: 0.5485 - val_accuracy: 0.8370\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 43s 949ms/step - loss: 0.4055 - accuracy: 0.8787 - val_loss: 0.7833 - val_accuracy: 0.8025\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 47s 1s/step - loss: 0.3406 - accuracy: 0.8992 - val_loss: 0.6002 - val_accuracy: 0.8276\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 39s 867ms/step - loss: 0.2992 - accuracy: 0.9118 - val_loss: 0.6473 - val_accuracy: 0.8433\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 37s 821ms/step - loss: 0.2493 - accuracy: 0.9306 - val_loss: 1.0725 - val_accuracy: 0.7931\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 52s 1s/step - loss: 0.2243 - accuracy: 0.9344 - val_loss: 0.5696 - val_accuracy: 0.8558\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 58s 1s/step - loss: 0.2107 - accuracy: 0.9369 - val_loss: 0.6074 - val_accuracy: 0.8433\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 45s 994ms/step - loss: 0.1776 - accuracy: 0.9494 - val_loss: 0.4685 - val_accuracy: 0.8777\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 40s 895ms/step - loss: 0.1503 - accuracy: 0.9536 - val_loss: 0.4887 - val_accuracy: 0.8809\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.1313 - accuracy: 0.9630 - val_loss: 0.4940 - val_accuracy: 0.8746\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 43s 962ms/step - loss: 0.1153 - accuracy: 0.9651 - val_loss: 0.6921 - val_accuracy: 0.8495\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 43s 963ms/step - loss: 0.1323 - accuracy: 0.9634 - val_loss: 0.5770 - val_accuracy: 0.8715\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 44s 987ms/step - loss: 0.1192 - accuracy: 0.9665 - val_loss: 0.5861 - val_accuracy: 0.8401\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 44s 971ms/step - loss: 0.1012 - accuracy: 0.9683 - val_loss: 0.7721 - val_accuracy: 0.8495\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 44s 985ms/step - loss: 0.1126 - accuracy: 0.9655 - val_loss: 0.5038 - val_accuracy: 0.8903\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 46s 1s/step - loss: 0.1000 - accuracy: 0.9669 - val_loss: 0.8347 - val_accuracy: 0.8276\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 45s 1s/step - loss: 0.0862 - accuracy: 0.9759 - val_loss: 0.6576 - val_accuracy: 0.8527\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train_encoded, \n",
    "                    epochs=20,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.1,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6947717994034828, 0.8828771483131763)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "preds = model.predict(X_val)\n",
    "preds_argmaxed = np.apply_along_axis(np.argmax, 1, preds)\n",
    "f1_score(y_val,preds_argmaxed, average='macro'), accuracy_score(y_val, preds_argmaxed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1025,    0,    0,    0,    2],\n",
       "       [   8,    6,    3,    0,    7],\n",
       "       [   8,    0,   33,    2,   10],\n",
       "       [  39,    0,    2,   43,   13],\n",
       "       [  87,    0,    0,    3,  280]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_val, preds_argmaxed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating predictions for file:  validation_images/UDPYYD.png\n",
      "Starting sliding window to create patches of size:  256 x 256 .\n",
      "Still processing, reached patch 10000\n",
      "Execution time for the last 10.000 patches:  0.014511823654174805  seconds.\n",
      "Processing continues...\n",
      "Finished preprocessing of the patches.\n",
      "Running patches through the Convnet...\n",
      "Finished predictions, execution time:  41.84871816635132  seconds.\n",
      "\n",
      "Saved predictions for file:  validation_images/UDPYYD.png \n",
      "\n",
      "Elapsed time:  44.858054876327515  seconds.\n",
      "\n",
      "\n",
      "Creating predictions for file:  validation_images/L7CT2I.png\n",
      "Starting sliding window to create patches of size:  256 x 256 .\n",
      "Still processing, reached patch 10000\n",
      "Execution time for the last 10.000 patches:  0.21338701248168945  seconds.\n",
      "Processing continues...\n",
      "Finished preprocessing of the patches.\n",
      "Running patches through the Convnet...\n",
      "Finished predictions, execution time:  43.261733055114746  seconds.\n",
      "\n",
      "Saved predictions for file:  validation_images/L7CT2I.png \n",
      "\n",
      "Elapsed time:  46.63918900489807  seconds.\n",
      "\n",
      "\n",
      "Creating predictions for file:  validation_images/DQIMQN.png\n",
      "Starting sliding window to create patches of size:  256 x 256 .\n",
      "Still processing, reached patch 10000\n",
      "Execution time for the last 10.000 patches:  0.2725839614868164  seconds.\n",
      "Processing continues...\n",
      "Finished preprocessing of the patches.\n",
      "Running patches through the Convnet...\n",
      "Finished predictions, execution time:  64.3893449306488  seconds.\n",
      "\n",
      "Saved predictions for file:  validation_images/DQIMQN.png \n",
      "\n",
      "Elapsed time:  68.79027509689331  seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processData.makePredictions(\"validation_images\", convnet=model, stepSize=64, windowSize=(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3503"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "preprocessed_patches = None\n",
    "del preprocessed_patches\n",
    "patch_coordinates = None\n",
    "del patch_coordinates\n",
    "X_train = None\n",
    "del X_train\n",
    "X_val = None \n",
    "del X_val\n",
    "y_train = None\n",
    "del y_train\n",
    "y_val = None\n",
    "training_data = None\n",
    "del training_data\n",
    "X_train_preprocessed = None\n",
    "del X_train_preprocessed\n",
    "predictions_array = None\n",
    "del predictions_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating suppressed csv for file:  validation_images/DQIMQN_prediction_suppressed.csv ...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/geo_py/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-656844fcde85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonMaxSuppressBoundingBoxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_images/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/da2-group-3/processData.py\u001b[0m in \u001b[0;36mnonMaxSuppressBoundingBoxes\u001b[0;34m(path, iou_threshold, score_threshold)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 original_predictions[\"label\"] == pred_class]\n\u001b[1;32m    167\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_original_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_original_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mcoordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_original_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/geo_py/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/geo_py/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "processData.nonMaxSuppressBoundingBoxes(\"validation_images/\", iou_threshold=0.0, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Images with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving / printing file:  L7CT2I _annotated.jpg\n",
      "Saving / printing file:  DQIMQN _annotated.jpg\n"
     ]
    }
   ],
   "source": [
    "drawImages.saveOrPrintImages(path=\"./validation_images\", print_to_output=False, valBoundingBoxes=True,saveImagesPath=\"./validation_images\", thickness=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'L7CT2I.png', 'tp': 1, 'fn': 7, 'fp': 4, 'f1': 0.153846155147929}\n",
      "{'file': 'DQIMQN.png', 'tp': 0, 'fn': 30, 'fp': 0, 'f1': 6.666666662222222e-10}\n"
     ]
    }
   ],
   "source": [
    "import csv, json, glob, cv2, random, os\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def calc_performance(gt_path, pred_path, image_name=None, verbose=0):\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "\n",
    "    # Create default performance values\n",
    "    performances = {\n",
    "        'file': image_name,\n",
    "        'tp': 0,\n",
    "        'fn': 0,\n",
    "        'fp': 0,\n",
    "        'f1': 0,\n",
    "    }\n",
    "\n",
    "    ## Load ground truth\n",
    "    with open(gt_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            row = {k: int(row[k]) if k != 'label' else row[k] for k in row.keys()}\n",
    "            ground_truth.append(row)\n",
    "\n",
    "    ## load predictions if path exists\n",
    "    if os.path.exists(pred_path):\n",
    "        with open(pred_path) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                row = {k: int(row[k]) if k != 'label' else row[k] for k in row.keys()}\n",
    "                predictions.append(row)\n",
    "\n",
    "    # Number of false positives equals number of left predictions\n",
    "    performances['fp'] = max(len(predictions) - len(ground_truth), 0)\n",
    "\n",
    "    for j, gt in enumerate(ground_truth):\n",
    "        gt_box = Polygon([(gt['y_upper_left'],  gt['x_upper_left']),\n",
    "                          (gt['y_upper_left'],  gt['x_lower_right']),\n",
    "                          (gt['y_lower_right'], gt['x_lower_right']),\n",
    "                          (gt['y_lower_right'], gt['x_upper_left'])])\n",
    "\n",
    "        if gt_box.area != (256. * 256.):\n",
    "            print(f'### Warning {j}: false ground truth shape of {gt_box.area} detected in {image_name}!')\n",
    "            print(gt['y_lower_right'] - gt['y_upper_left'], gt['x_lower_right'] - gt['x_upper_left'])\n",
    "\n",
    "        best_found_iou = (None, 0.) # (idx, IoU)\n",
    "        for i, pred in enumerate(predictions):\n",
    "            if gt['label'] == pred['label']:\n",
    "                pred_box = Polygon([(pred['y_upper_left'],  pred['x_upper_left']),\n",
    "                                    (pred['y_upper_left'],  pred['x_lower_right']),\n",
    "                                    (pred['y_lower_right'], pred['x_lower_right']),\n",
    "                                    (pred['y_lower_right'], pred['x_upper_left'])])\n",
    "\n",
    "                if pred_box.area != (256. * 256.):\n",
    "                    print(f'### Warning {i}: false predicted shape of {pred_box.area} detected in {image_name}!')\n",
    "                    print(pred['y_lower_right'] - pred['y_upper_left'], pred['x_lower_right'] - pred['x_upper_left'])\n",
    "\n",
    "                ## Calculate IoU\n",
    "                next_iou = (gt_box.intersection(pred_box).area + 1) / (gt_box.union(pred_box).area + 1)\n",
    "\n",
    "                # If the next found IoU is larger than the previous found IoU -> override\n",
    "                if next_iou > best_found_iou[1]:\n",
    "                    best_found_iou = (i, next_iou)\n",
    "\n",
    "        ## Append metric. If IoU is larger 0.5, then its a true positive, else false negative\n",
    "        if best_found_iou[0] is not None and best_found_iou[1] >= 0.5:\n",
    "            del predictions[best_found_iou[0]] # Remove prediction from list!\n",
    "            performances['tp'] += 1 # Increase number of True Positives\n",
    "            if verbose == 1:\n",
    "                print(f'Found correct prediction with IoU of {round(best_found_iou[1], 3)} and label {gt[\"label\"]}!')\n",
    "        else:\n",
    "            performances['fn'] += 1 # Increase number of False Negatives\n",
    "            if verbose == 1:\n",
    "                print(f'Found false prediction with IoU of {round(best_found_iou[1], 3)} and label {gt[\"label\"]}!')\n",
    "\n",
    "    ## Calculate F1-Score\n",
    "    performances['f1'] = (performances['tp'] + 1e-8) / \\\n",
    "                                    (performances['tp'] + 0.5 * (performances['fp'] + performances['fn']) + 1e-8)\n",
    "    return performances\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = 'validation_images' # Change if needed\n",
    "\n",
    "    ## Iterate over all validation images\n",
    "    for image_path in glob.glob(path + '/*.png'):\n",
    "        image_name = image_path.split('/')[-1]\n",
    "        gt_path = image_path[:-4] + '_true.csv' # Ground Truth path\n",
    "        pred_path = image_path[:-4] + '_prediction_suppressed.csv' # Prediction path\n",
    "        performance = calc_performance(gt_path, pred_path, image_name)\n",
    "        print(performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('geo_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fb245b132a2be47283fe5f25201c6b65dd2f54345d845219426bdf4f2ce9823"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
